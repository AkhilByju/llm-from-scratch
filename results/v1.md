# Version 1 Experiments

This document details experiments for **v1.0 → v1.2**, including training setup, hyperparameters, logs, results, and qualitative observations.

---

## v1.0 — Baseline Character Tokenization

### Hyperparameters

```python
batch_size = 16
block_size = 128
max_iters = 5000
eval_interval = 500
learning_rate = 3e-4
device = 'cuda' if torch.cuda.is_available() else 'cpu'
eval_iters = 200
n_embd = 128
n_head = 4
n_layer = 4
dropout = 0.2
```

### Training Performance

- No detailed metrics were recorded
- This run primarily serves as an initial baseline to make sure the training pipeline works end to end

### Observations

- Model trained without major runtime issues.
- No qualitative evaluation or quantitative logging performed.
- Functioned as a sanity check before more systematic experiments in later versions.

## v1.1 — Scaled Embedding Layers and training time

### Hyperparameters

```python
batch_size = 16
block_size = 256
max_iters = 10000
eval_interval = 500
learning_rate = 3e-4
device = "mps" if torch.backends.mps.is_available() else "cpu"
eval_iters = 200
n_embd = 256
n_head = 8
n_layer = 6
dropout = 0.2
```

### Training Performance

- Best validation loss: **1.3574**
- Best training loss: **1.3309**
- Maybe could have trained for longer as validation loss was still steadily decreasing

### Images

![Training log](images/v1.1_log.png)

### Observations

> _The model was able to generate many real English words, showing that character-level tokenization allowed it to piece together recognizable vocabulary. However, the output lacked coherent sentence structure, instead producing a stream of loosely related phrases. This indicates early success in forming words, but highlights the difficulty of scaling character-level models to produce meaningful syntax._

---

## v1.2 — Adding ALiBi + GeLU

### Hyperparameters

```python
batch_size = 64
block_size = 512
max_iters = 2000
learning_rate = 3e-4
n_embd = 256
n_head = 8
n_layer = 6
dropout = 0.2
```

### Training Performance

- Hit **1.45 loss** at 900 iterations (plateaued early)
- Faster convergence than v1.1

**Plots & Logs:**
![Training log](imgs/v1.2_log.png)
![Validation curve](imgs/v1.2_curve.png)

### Observations

- ALiBi allowed longer context without retraining position embeddings
- GeLU activation gave smoother convergence
- Plateaued earlier than expected → might need LR scheduling

---

## Additional Notes

- Compare across v1.0 → v1.2: best trade-off was v1.1 (generalization vs. stability).
- Next step (v2): move to BPE tokenization and transformer-style scaling.

---

## References

- [ALiBi: Train Short, Test Long](https://arxiv.org/abs/2108.12409)
- [Gaussian Error Linear Units (GeLU)](https://arxiv.org/abs/1606.08415)

```

```
