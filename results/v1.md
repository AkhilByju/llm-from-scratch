# Version 1 Experiments

This document details experiments for **v1.0 → v1.2**, including training setup, hyperparameters, logs, results, and qualitative observations.

---

## v1.0 — Baseline Character Tokenization

### Hyperparameters

```python
batch_size = 16
block_size = 128
max_iters = 5000
eval_interval = 500
learning_rate = 3e-4
device = 'cuda' if torch.cuda.is_available() else 'cpu'
eval_iters = 200
n_embd = 128
n_head = 4
n_layer = 4
dropout = 0.2
```

### Training Performance

- No detailed metrics were recorded
- This run primarily serves as an initial baseline to make sure the training pipeline works end to end

### Observations

- None

## v1.1 — Improved Tokenizer & Dropout

### Hyperparameters

```python
batch_size = 64
block_size = 256
max_iters = 1500
learning_rate = 3e-4
n_embd = 256
n_head = 6
n_layer = 6
dropout = 0.2
```

### Training Performance

- Best validation loss: **X.XXX**
- Improved stability with higher embedding dimension

**Plots:**
![Training curve](imgs/v1.1_loss.png)

### Observations

- Training stabilized compared to v1.0
- Dropout reduced overfitting on validation set

---

## v1.2 — Adding ALiBi + GeLU

### Hyperparameters

```python
batch_size = 64
block_size = 512
max_iters = 2000
learning_rate = 3e-4
n_embd = 256
n_head = 8
n_layer = 6
dropout = 0.2
```

### Training Performance

- Hit **1.45 loss** at 900 iterations (plateaued early)
- Faster convergence than v1.1

**Plots & Logs:**
![Training log](imgs/v1.2_log.png)
![Validation curve](imgs/v1.2_curve.png)

### Observations

- ALiBi allowed longer context without retraining position embeddings
- GeLU activation gave smoother convergence
- Plateaued earlier than expected → might need LR scheduling

---

## Additional Notes

- Compare across v1.0 → v1.2: best trade-off was v1.1 (generalization vs. stability).
- Next step (v2): move to BPE tokenization and transformer-style scaling.

---

## References

- [ALiBi: Train Short, Test Long](https://arxiv.org/abs/2108.12409)
- [Gaussian Error Linear Units (GeLU)](https://arxiv.org/abs/1606.08415)

```

```
