# LLM From Scratch

Goal: Implement a transformer-based LLM completely form scratch
Motivation: learn internals of tokenization, training loops, attention mechanisms, etc.
Scopre: pretraining on WikiText, experimenting with hyperparameters
