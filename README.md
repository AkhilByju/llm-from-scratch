# LLM From Scratch

Goal: Implement a transformer-based LLM completely form scratch \
Motivation: learn internals of tokenization, training loops, attention mechanisms, etc. \
Scope: pretraining on WikiText, experimenting with hyperparameters \

## Table of Contents

- [Motivation](#motivation)
- [Whatâ€™s in this repo](#whats-in-this-repo)
- [Versions / Roadmap](#versions--roadmap)
- [Setup / Requirements](#setup--requirements)
- [How to use](#how-to-use)
- [Design & Architecture](#design--architecture)
- [Key Experiments & Results](#key-experiments--results)
- [Challenges & Learnings](#challenges--learnings)
- [How to Contribute / Expand](#how-to-contribute--expand)
- [License](#license)
- [References](#references)

## Motivation

- I want to learn and explore Large Language Models
- Understand tokenization and vocobulation methods
- Build progressively more capable models to better understand LLMs
- Compare different architectures and positional encodings (e.g., ALiBi vs. RoPE)

## What's in this repo

### These are the main models and each has their own iterative improvements

- `v0.py` - baseline model with character tokenization
- `v1.py` - making the transformer more robust (still with character tokenization)
- `v2.py` - BPE tokenization
- `v3.py` - BPE tokenization but only with an English Vocabulary

## Versions / Roadmap

## Setup / Requirements

## How to use

## Design & Architecture

## Key Experiments & Results

## Challenges & Learnings

## How to Contribute / Expand

## Liscence

## References
